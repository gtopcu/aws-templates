
------------------------------------------------------------------------------------
SSH Connection:
------------------------------------------------------------------------------------
chmod 400 gt-key.pem
ssh -i "gt-key.pem" ec2-user@54.78.7.114

------------------------------------------------------------------------------------
Install CLI 
------------------------------------------------------------------------------------
https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html
brew install awscli
curl "https://awscli.amazonaws.com/AWSCLIV2.pkg" -o "AWSCLIV2.pkg"
sudo installer -pkg AWSCLIV2.pkg -target /
apt install awscli

CLI Doc:
https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-files.html
https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-profiles.html

CLI Skeleton:
https://docs.aws.amazon.com/cli/latest/userguide/cli-usage-skeleton.html

sudo chown -R hukanege /Users/hukanege/.aws/
/.aws/credentials
/.aws/config

[default]
aws_access_key_id=YOUR_ACCESS_KEY_HERE
aws_secret_access_key=YOUR_SECRET_ACCESS_KEY
region=YOUR_REGION

aws --version (aws-cli/2.22.26 Python/3.12.6 Darwin/24.2.0 exe/x86_64)
aws configure
aws configure [--profile profile-name]
aws configure list
aws configure list-profiles
aws configure set region us-east-2 --profile root
aws configure import --csv file://credentials.csv
aws iam get-user
which aws

------------------------------------------------------------------------------------
DynamoDB:
https://docs.aws.amazon.com/cli/latest/reference/dynamodb/index.html#cli-aws-dynamodb
------------------------------------------------------------------------------------

aws dynamodb list-tables \
    --profile spiky \
    --generate-cli-skeleton input > tables.json 

aws dynamodb describe-continuous-backups \
    --table-name Blog-gkyef3d5qrfxfcoyt76zuf7n4i-dev \
    --profile spiky

aws dynamodb update-continuous-backups \
    --table-name Blog-gkyef3d5qrfxfcoyt76zuf7n4i-dev \
    --point-in-time-recovery-specification PointInTimeRecoveryEnabled=true \
    --profile spiky

aws dynamodb update-continuous-backups \
    --generate-cli-skeleton input > tablesSkeleton.json

aws dynamodb update-continuous-backups \
    --profile spiky \
    --cli-input-json file://tables.json


------------------------------------------------------------------------------------
S3:
https://docs.aws.amazon.com/cli/latest/userguide/cli-services-s3-commands.html
------------------------------------------------------------------------------------

aws s3 ls
aws s3 mb s3://mybucket
aws s3 rb s3://mybucket
aws s3 rm s3://mybucket --recursive -> Remove all objects from the bucket(does not delete older versions/delete markers)

(s3 cp & sync -> multipart, s3api copy is not multipart)
aws s3 cp <file> s3://<s3path> --storage-class [CLASS] 
aws s3 cp s3://gokhantopcu .
aws s3 sync <dir> s3://<s3path>

aws s3 mv s3://source_folder/ s3://destination_folder/ --recursive
aws s3 cp s3://source_folder/ s3://destination_folder/ --recursive
aws s3 rm s3://source_folder --recursive

sync:
- Only copy new or modified files
- Skip files that already exist and haven't changed
- Recursively handle all subdirectories
- Automatically handle large files with multipart uploads

aws s3 sync . s3://mybucket/myfolder
aws s3 sync . s3://mybucket/myfolder --delete 
aws s3 sync . s3://mybucket/myfolder --exclude="*" --include="[0-9]*.jpg"

--delete: Delete files in the destination that don't exist in the source
--exclude "*.tmp": Exclude files matching a pattern
--include "*.jpg": Include only specific files
--dryrun: Preview what will be transferred without actually doing it
--only-show-errors: Reduce output by only showing errors(if there're many files)

Configure S3 CLI settings for better performance
aws configure set default.s3.max_concurrent_requests 128
aws configure set default.s3.max_queue_size 10000
aws configure set default.s3.multipart_threshold 64MB
aws configure set default.s3.multipart_chunksize 16MB

Run multiple syncs in parallel based on prefixes
aws s3 sync source/ destination/ --exclude "*" --include "0*" --include "1*" &
aws s3 sync source/ destination/ --exclude "*" --include "2*" --include "3*" &
aws s3 sync source/ destination/ --exclude "*" --include "4*" --include "5*" &

AWS DataSync - for Large-Scale Transfers
- Designed specifically for large-scale transfers
- Up to 10x faster than standard tools
- Automatically handles validation and network optimization
- Maintains file metadata and permissions
- Better for transfers of hundreds of terabytes and millions of files

-------------------------------------------------------------------------------------------------------------------------

aws s3api create-bucket --bucket $bucket_name --region $region --create-bucket-configuration LocationConstraint=$region
aws s3api list-objects --bucket <YOUR_BUCKET_NAME> --page-size 5 (default: 1000)
aws s3api list-objects --bucket <YOUR_BUCKET_NAME> --max-items 1
aws s3api get-bucket-policy-status --bucket testdgcloudfronts3

aws s3api copy-object --copy-source $s3prefix/$objectname --key $objectname 
    --bucket $s3prefix --region $region --storage-class $s3class

aws s3api put-object --bucket [target-bucket] \
--key tags.txt \
--body "makeyourtime.txt" \
--tagging "AllYourTags=AreBelong&To=Us"
--object-lock-retain-until-date "2099-01-01T00:00:00+0000" \
--object-lock-legal-hold-status "ON" \
--object-lock-mode "COMPLIANCE"

aws s3api put-bucket-versioning --bucket DOC-EXAMPLE-BUCKET1 &&
    --versioning-configuration Status=Enabled,MFADelete=Enabled --mfa "SERIAL 123456"

aws s3api list-multipart-uploads --bucket [bucket-name]
aws s3api create-multipart-upload --bucket [bucket-name] --key [key]

aws s3api upload-part \
--bucket [bucket-name] \
--key [key] \
--upload-id xxx \
--part-number 1 \
--body somelocalfile.txt

curl -X GET https://[bucketname].s3.amazonaws.com
curl -X DELETE https://[bucketname].s3-ap-southeast-2.amazonaws.com


-------------------------------------------------------------------------------------------------------------------------

# List empty buckets
for bucket in $(aws s3 ls | awk '{print $3}'); do
  count=$(aws s3 ls s3://$bucket --recursive | wc -l)
  if [ $count -eq 0 ]; then
    echo "$bucket"
  fi
done


------------------------------------------------------------------------------------
EC2:
https://docs.aws.amazon.com/cli/latest/userguide/cli-services-ec2.html
------------------------------------------------------------------------------------

aws ec2 describe-instances
aws ec2 stop-instances --instance-ids i-0744af96f9e05dfe5
aws ec2 create-snapshot
aws ec2 run-instances --cli-input-json file://ec2runinst.json

Display EC2 user & metada:
------------------------------------------------------------
curl http://169.254.169.254/latest/user-data
curl http://169.254.169.254/latest/meta-data
curl http://169.254.169.254/latest/meta-data/local-hostname
curl http://169.254.169.254/latest/meta-data/public-ipv4
...


------------------------------------------------------------------------------------
Lambda
------------------------------------------------------------------------------------

aws lambda list-functions --function-version ALL --region <REGION> \
    --output text --query "Functions[?Runtime=='<RUNTIME>'].FunctionArn"  

aws lambda update-function-code --function-name pythonLambda --zip-file fileb://deployment_package.zip

aws lambda list-layers --compatible-runtime python3.13
aws lambda list-layer-versions --layer-name my-layer

aws lambda publish-layer-version --layer-name $LAYER_NAME --description "pymediainfo" \
    --content S3Bucket=$LAMBDA_LAYERS_BUCKET,S3Key=pymediainfo-python38.zip --compatible-runtimes python3.8

 aws lambda publish-layer-version --layer-name my-layer \
    --description "My layer" \
    --license-info "MIT" \
    --zip-file fileb://layer.zip \
    --compatible-runtimes python3.10 python3.11 \
    --compatible-architectures "arm64" "x86_64"   

aws lambda delete-layer-version --layer-name my-layer --version-number 1

aws lambda invoke --function-name my-function output.json && cat output.json 

aws lambda invoke \
    --function-name my-function \
    --cli-binary-format raw-in-base64-out \
    --payload '{"name": "Alice", "birthday": "1990-01-01", "email": "alice@gmail.com"}' \
    output.json && cat output.json 

------------------------------------------------------------------------------------
ECS
------------------------------------------------------------------------------------
cluster_arn=$(aws ecs list-clusters | jq -r '.clusterArns[] | select(contains("container-demo"))')
clustername=$(aws ecs describe-clusters --clusters $cluster_arn | jq -r '.clusters[].clusterName')
aws ecs update-cluster-settings --cluster ${clustername}  --settings name=containerInsights,value=enabled --region ${AWS_REGION}

aws ecs list-clusters
aws ecs describe-clusters
aws ecs update-cluster-settings --cluster sddg-ecs-cluster-dev --settings name=containerInsights,value=enabled --region us-east-2 --profile spiky

------------------------------------------------------------------------------------
------------------------------------------------------------------------------------
